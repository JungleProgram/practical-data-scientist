{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6.3: Ensemble Methods\n",
    "\n",
    "This lecture, we are going to train and compare a random forest and a xgboost model on a real dataset.\n",
    "\n",
    "**Learning goals:**\n",
    "- train a random forest classifier\n",
    "- train an adaboost classifier\n",
    "- visualize and compare the model decision boundaries\n",
    "- analyse the effect of regularization parameters\n",
    "- train a random forest regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Let's try to improve our fake banknote detector from lecture 5.3. üïµÔ∏è‚Äç‚ôÄÔ∏è We'll use the same [banknote authentication dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication), and try to solve the fake/genuine classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "### 2.1 Data Munging\n",
    "\n",
    "Let's load our `.csv` into a pandas `DataFrame`, and have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank_note.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are dealing with 4 features, and one binary label. The features are standardized, so no further preprocessing is necessary.\n",
    "\n",
    "We can create our feature matrix, `X`, and our label vector, `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['feature_2', 'feature_4']].values\n",
    "y = df['is_fake'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the dataset to remember the complexity of the classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=0.5)\n",
    "ax.set_xlabel('feature_2')\n",
    "ax.set_ylabel('feature_4')\n",
    "ax.set_title('Banknote Classification')\n",
    "handles, labels = scatter.legend_elements()\n",
    "ax.legend(handles=handles, labels=['genuine', 'fake']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important aspects to notice are:\n",
    "* the data is _not separable_\n",
    "* the relationship between `feature_2` and `feature_4` is _non-linear_\n",
    "\n",
    "Just like last lecture, this should be a good test for our ensemble models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†2.2 Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Random Forests\n",
    "\n",
    "`sklearn` separates random forest models for classification and regression. For this binary classification task, let's train a `RandomForestClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=0)\n",
    "forest_clf = forest_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Can you list all the steps that sklearn had to go through to train this random forest? Take your time, there a lot of things going on in that `.fit()` function!\n",
    "\n",
    "Recall that random forests are an _ensemble_ of decision trees, and we can retrieve each tree with the `.estimators_` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This random forest is an ensemble of {len(forest_clf.estimators_)} decision trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.estimators_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have the space to visualize 100 decision tree flow chart, let's directly plot the random forest's decision boundary with our helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_decision_boundary(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    plot_decision_boundary(ax, clf, xx, yy, **params)\n",
    "\n",
    "\n",
    "def plot_classification(ax, X, y, clf):\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    scatter = ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=1.0)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title('Bank Notes Classification')\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    ax.legend(handles=handles, labels=['genuine', 'fake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "plot_classification(ax, X, y, forest_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is considerably different from the single decision tree. It is still only made of vertical and horizontal lines, but this time it is far more _detailed_. This is because these are the combined lines of 100 decision boundaries.\n",
    "\n",
    "Recall that these predictions are made by majority voting. For each point in this graph, all the predictions \"votes\" from the 100 decision trees are rounded up: \n",
    "- those where there are more `fake` votes than `genuine` are shown in red\n",
    "- those where there are more `genuine` than `fake` votes, are shown in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_clf = AdaBoostClassifier(random_state=0)\n",
    "ab_clf = ab_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Can you list all the steps that sklearn had to go through to train this boosted ensemble? Take your time, there a lot of things going on in that `.fit()` function!\n",
    "\n",
    "Just like random forests, we can retrieve the different base models from the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'This AdaBoost model is an ensemble of {len(ab_clf.estimators_)} decision trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_clf.estimators_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this estimator has `max_depth=1`: it is a \"decision stump\", i.e a very underfit model which will shine when boosted! ‚ú®\n",
    "\n",
    "In fact we can visualise the decision boundary of a few of our decision stumps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_values = [0, 1, 2, 3]\n",
    "decision_stumps = ab_clf.estimators_[:4]\n",
    "titles = [f'estimator #{n}' for n in n_estimators_values]\n",
    "\n",
    "compare_classification(X, y, decision_stumps, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to imagine how this could combine into an accurate non-linear model... let's visualise the decision boundary of our boosted ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "plot_classification(ax, X, y, ab_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the decision boundary is very much _not_ linear! By combining the decision stumps with different weights, the boosted ensemble effectively lets them \"focus\" on different areas of the dataset. And the result is decent üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our models by asking them to predict a banknote in the small `genuine` cluster on the left hand side of the graphs above.  We'll use $feature\\_1 = -1; feature\\_2 = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.array([-1, 0]).reshape(1, 2)\n",
    "print(f'Features: {x_predict}')\n",
    "\n",
    "forest_clf_prediction = forest_clf.predict(x_predict)\n",
    "print(f'Random Forest prediction: {forest_clf_prediction}')\n",
    "\n",
    "\n",
    "ab_clf_prediction = ab_clf.predict(x_predict)\n",
    "print(f'AdaBoost prediction: {ab_clf_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysis\n",
    "\n",
    "#### 2.4.1 Regularization: number of decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important hyperparameter in the training of a random forest is the number of decision trees which form the ensemble. In sklearn, this is controlled with the `n_estimators` argument. Let's check out its effects on the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(X, y, **kwargs):\n",
    "    clf = RandomForestClassifier(random_state=0, **kwargs)\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "n_estimators_values = [3, 10, 100]\n",
    "forests = [train_forest(X, y, n_estimators=n) for n in n_estimators_values]\n",
    "titles = [f'n_estimators={n}' for n in n_estimators_values]\n",
    "\n",
    "compare_classification(X, y, forests, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigger ensemble means less overfitting, and a more accurate combined model. However, more `n_estimators` increases the training time! Try training a random forest with 5000 decision trees and count the seconds tick away üò™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_forest(X, y, n_estimators=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Regularization: minimum samples per leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí™üí™ Investigate the effect of the `min_samples_leaf` argument on a random forest.\n",
    "- you can use the exact same code structure as the section above\n",
    "- you don't have to redefine the function `.train_forest()`, since `**kwargs` will work with any _named argument_.\n",
    "- pick a suitable range of parameter values. You can always change them and run the cell again!\n",
    "- the unit test is having nice looking graphs üôÉ\n",
    "\n",
    "\n",
    "üß† Define the effect of the `min_samples_leaf` parameter. It might help to check out the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß†üß† How does `min_samples_leaf` affect the model's generalization? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Regularization: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of `n_estimators`, the regularization of boosted ensembles can be controlled with `learning_rate`. This shapes the weights applied to each estimator at each boosting iteration. Higher learning rates increase the contribution of each classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adaboost(X, y, **kwargs):\n",
    "    clf = AdaBoostClassifier(random_state=0, **kwargs)\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "n_estimators_values = [0.01, 0.1, 1]\n",
    "adaboosts = [train_adaboost(X, y, learning_rate=n, n_estimators=500) for n in n_estimators_values]\n",
    "titles = [f'learning_rate={n}' for n in n_estimators_values]\n",
    "\n",
    "compare_classification(X, y, adaboosts, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that lower learning rates act as a _regularisation_ method, but too much regularisation affects the accuracy of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "\n",
    "As mentioned in the lecture slides, decision trees can solve regression tasks. They do so by using _variance reduction_ instead of _homogeneity metrics_ to split each node, and by assigning _numerical values_ to each leaf node.\n",
    "\n",
    "We'd like to try this out this on our \"instagram planning\" dataset, and aim to predict the `actual_minutes` spent online from the originally `planned_minutes` (see notebook 3.7 for more a more detailed exploration of this dataset).\n",
    "\n",
    "Except this time, _you_ are going to compare and analyse these regression models! \n",
    "\n",
    "üí™üí™üí™ Train, analyse a decision tree regressor & a random forest regressor on the instagram planning dataset. Some helper functions are supplied so you can focus on the machine learning bits üòé. Here's a list of the steps you should be taking to lead your analysis:\n",
    "- load the `instagram_planning.csv` dataset into a `DataFrame`\n",
    "- optionally visualize this dataset to refresh your memory\n",
    "- create a feature matrix, `X`, and a label vector, `y`\n",
    "- fit a [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) model to the data (check the official documentation for details)\n",
    "- optionally visualize the decision tree's nodes to understand its prediction logic with `.plot_tree()`\n",
    "- fit a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) model to the data (check the official documentation for details)\n",
    "- plot and compare their decision boundaries with `.compare_regression()`, which has the exact same interface as `.compare_classification()`.\n",
    "- the unit test is having nice looking graphs üôÉ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(ax, X, y, reg):\n",
    "\n",
    "    # plot the examples\n",
    "    ax.scatter(X, y, alpha=0.6)\n",
    "\n",
    "    # create feature matrix\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    x_line = np.linspace(xmin, xmax, 30).reshape(-1, 1)\n",
    "    \n",
    "    # predict\n",
    "    y_line = reg.predict(x_line)\n",
    "\n",
    "    # plot the hypothesis\n",
    "    ax.plot(x_line, y_line, c='g', linewidth=3)\n",
    "\n",
    "    # formatting\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xlabel('planned online time (min)')\n",
    "    ax.set_ylabel('time spent online (min)')\n",
    "    ax.set_title('Online Procrastination');\n",
    "    \n",
    "def compare_regression(X, y, regs, titles):\n",
    "    fig = plt.figure(figsize=(14, 4), dpi=100)\n",
    "    for i, reg in enumerate(regs):\n",
    "        ax = fig.add_subplot(1, len(regs), i+1)\n",
    "        plot_regression(ax, X, y, reg)\n",
    "        ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Take your time to think about what happens in the `.fit()` method of the `DecisionTreeRegressor`. Can you list the main similarities and differences with a decision tree classifier?\n",
    "\n",
    "\n",
    "üß† When plotting the predictions of the decision tree and random forest with `.compare_regression()`, why are they so \"bumpy\" compare to linear or polynomial regression models?\n",
    "\n",
    "üß†üß† How are the predictions of each individual decision tree combined to make the numerical prediction of the random forest?\n",
    "\n",
    "üß† What do the `.compare_regression()` plots show about random forest regressors and regularization? \n",
    "\n",
    "üí™üí™ Feel free to investigate effect of various parameters on the regression models! You can use the same code structure as for the \"analysis\" section with the decision tree & random forest classifiers above. Remember that all parameters to play with are listed in the official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "Today we learned about **ensemble learning**, a method for training and combining **weak learners** into superpowered **ensemble models**. We first described **bagging**, which randomly averages overfit models to decrease their variance and improve their generalisation properties. **Random forests** are a successful example of a bagging ensemble of decision trees. We then studied **boosting**, which iteratively retrains underfit models focusing on their errors, and predicts using a weighted combination of the ensemble. **AdaBoost** is a simple example of boosted decision stumps. We applied these models to our banknote classification dataset, and introduced several **regularization** procedures: the number of estimators, the minimum samples per leaf, and the learning rate. Finally, we also tested ensemble methods on a **regression** task with our instagram_planning dataset.\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "\n",
    "- [sklearn documentation - ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)  \n",
    "Official documentation about ensemble methods in sklearn\n",
    "- [Introduction to random forests](https://victorzhou.com/blog/intro-to-random-forests/)  \n",
    "Excellent visual blogpost which explains random forests in detail\n",
    "- [Python data science handbook - random forests](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)  \n",
    "Practical code-along post about implementation of random forests with sklearn\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [random forest python](https://github.com/kevin-keraudren/randomforest-python)  \n",
    "Implementation of random forest from scratch in python\n",
    "- [Gentle Introduction to Gradient Boosting](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "- [Introduction to XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)  \n",
    "Comprehensive description of one of the most successful algorithms in data science: xgboost\n",
    "- [args and kwargs demystified](https://realpython.com/python-kwargs-and-args/)  \n",
    "blog post about \\*\\*kwargs in python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
