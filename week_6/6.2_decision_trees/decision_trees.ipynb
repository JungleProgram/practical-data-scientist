{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6.2: Decision Trees\n",
    "\n",
    "This lecture, we are going to train a decision tree on a real dataset.\n",
    "\n",
    "**Learning goals:**\n",
    "- train a decision tree classifier\n",
    "- visualize and compare the model decision boundary to previously seen models\n",
    "- analyze the effect of regularization parameter `max_depth`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Let's try to improve our fake banknote detector from lecture 6.1. üïµÔ∏è‚Äç‚ôÄÔ∏è We'll use the same [banknote authentication dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication), and try to solve the fake/genuine classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "### 2.1 Data Munging\n",
    "\n",
    "Let's load our `.csv` into a pandas `DataFrame`, and have a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:24:49.576293Z",
     "start_time": "2022-03-02T15:24:45.234149Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bank_note.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:24:49.596834Z",
     "start_time": "2022-03-02T15:24:49.578189Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are dealing with 4 features, and one binary label. The features are standardized, so no further preprocessing is necessary.\n",
    "\n",
    "We can create our feature matrix, `X`, and our label vector, `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:24:52.112925Z",
     "start_time": "2022-03-02T15:24:52.109727Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['feature_2', 'feature_4']].values\n",
    "y = df['is_fake'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the dataset to remember the complexity of the classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:24:58.466599Z",
     "start_time": "2022-03-02T15:24:58.207699Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=0.5)\n",
    "ax.set_xlabel('feature_2')\n",
    "ax.set_ylabel('feature_4')\n",
    "ax.set_title('Banknote Classification')\n",
    "handles, labels = scatter.legend_elements()\n",
    "ax.legend(handles=handles, labels=['genuine', 'fake']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is _not separable_ , and the relationship between `feature_2` and `feature_4` is _non-linear_. This should make a good challenge for our decision trees! üå≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†2.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†2.2 Training\n",
    "\n",
    "#### 2.2.1 Decision Trees\n",
    "\n",
    "We can use our favourite sklearn model api with `.fit` and `.predict()`. The class for decision tree classifiers is ... `DecisionTreeClassifier` üòè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:25:48.669911Z",
     "start_time": "2022-03-02T15:25:43.781497Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf = tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Can you list all the steps that sklearn had to go through to train this decision tree with the function `.fit()`? \n",
    "\n",
    "We usually try to investigate our model parameters after fitting the model. However, decision trees don't have a vector $\\theta$ or support vectors, they are _non-parametric models_. In the lecture slides, we introduced decision trees as nested if-statements. So instead, we can investigate their _decision node splits_.\n",
    "\n",
    "Let's check the _depth_ of our decision tree. Remember this is the maximal length of a decision _branch_. We can also output the total number of leaf nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:25:54.235051Z",
     "start_time": "2022-03-02T15:25:54.232147Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'This decision tree has depth {tree_clf.get_depth()}, and contains {tree_clf.get_n_leaves()} leaves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 decision levels, that's quite a big tree we've grown here! üå≥\n",
    "\n",
    "Recall that decision trees make predictions by stepping through their decision nodes. By visualizing our tree's nodes, we can interpret its predictions. Decision trees are sometimes called white box models, because we can examine their inner workings.\n",
    "\n",
    "We'll use sklearn's `.plot_tree()` method to visualize the decision nodes. We must _truncate_ the tree with a `max_depth=2`, or else the visualization will be too big to fit on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:26:11.344915Z",
     "start_time": "2022-03-02T15:26:10.894138Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(dpi=200)\n",
    "ax = fig.add_subplot(111)\n",
    "plot_tree(ax=ax, decision_tree=tree_clf, filled=True, max_depth=2, feature_names=['x1', 'x2'], rounded=True, precision=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is packed with information ü§§\n",
    "\n",
    "- The first line of each node defines its _split_. i.e The feature and the value which partitions incoming data into its children nodes.\n",
    "\n",
    "- The second line indicates the _gini impurity_ metric associated with each split on the training dataset. Remember, low gini impurity implies homogeneity and is therefore a good thing!\n",
    "\n",
    "- The third line displays the number of training examples belonging to that node.\n",
    "\n",
    "- The fourth line shows the _class split_ of this node. i.e How many genuine vs fake bills were present in this node during training. We expect this gap to get larger as we go deeper down the branches.\n",
    "\n",
    "- The node color represents the same information as the fourth line: bluer nodes contain more genuine bills, redder nodes contain more fake bills. We also expect these hues to get more pronounced closer to the leafs nodes.\n",
    "\n",
    "Note that all lines except the first tell information specific to _training_. All that is needed for prediction is the feature values of the splits, and the class attribution of each leaf node.\n",
    "\n",
    "üß† Take your time to understand this graph and how it was \"greedily\" built during training.\n",
    "\n",
    "These splits (if-statements) shape the model's decision boundary. We'd like to visualize this along with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily plot our decision tree classifier's predictions üé®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:28:12.486655Z",
     "start_time": "2022-03-02T15:28:12.474943Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_decision_boundary(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    plot_decision_boundary(ax, clf, xx, yy, **params)\n",
    "\n",
    "\n",
    "def plot_classification(ax, X, y, clf):\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    scatter = ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k', alpha=1.0)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title('Bank Notes Classification')\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    ax.legend(handles=handles, labels=['genuine', 'fake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:28:13.380711Z",
     "start_time": "2022-03-02T15:28:13.079941Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5), dpi=120)\n",
    "ax = fig.add_subplot()\n",
    "plot_classification(ax, X, y, tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a funky decision boundary! üò≥ It is made exclusively of vertical and horizontal lines because predictions are made from a succession of if-statements on single features. The very thin rectangular areas are typical of _overfit_ decision trees. They try to fit single data points instead of the underlying patterns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Comparison\n",
    "\n",
    "Let's compare this model with two other types of classifiers: logistic regression and an RBF kernel SVM (see lecture 6.1).\n",
    "\n",
    "Just like last lecture, we can write a small loop to visualize these models next to eachother. We'll then train a non-linear SVM to compare with the decision tree and the random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T15:30:16.654252Z",
     "start_time": "2022-03-02T15:30:14.753237Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_classification(X, y, clfs, titles):\n",
    "    fig = plt.figure(figsize=(14, 4), dpi=100)\n",
    "    for i, clf in enumerate(clfs):\n",
    "        ax = fig.add_subplot(1, len(clfs), i+1)\n",
    "        plot_classification(ax, X, y, clf)\n",
    "        ax.set_title(titles[i])\n",
    "        \n",
    "        \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=0)\n",
    "svm_rbf = svm_rbf.fit(X, y)\n",
    "\n",
    "compare_classification(X, y, [log_reg, svm_rbf, tree_clf], ['RBF SVM', 'Naive Bayes', 'Decision Tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models on the right exhibit non-linear decision boundaries, but in very different ways. \n",
    "- the SVM is busy maximizing the margin shaped by its distance features, creating smooth \"blobs\" that follow the edges of the data\n",
    "- the decision tree tries its best to split the dataset with single feature thresholds, splitting the data into nested rectangles\n",
    "\n",
    "\n",
    "üß†üß† None of these models seem to be able to correctly predict the fake examples near $[1, -1]$. Why is that?\n",
    "\n",
    "üß†üß† In your opinion, which is the algorithm most adapted to this dataset and classification task? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our models by asking them to predict a banknote in the small `genuine` cluster on the left hand side of the graphs above.  We'll use $feature\\_1 = -1; feature\\_2 = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:13:43.184102Z",
     "start_time": "2022-03-02T16:13:43.178571Z"
    }
   },
   "outputs": [],
   "source": [
    "x_predict = np.array([-1, 0]).reshape(1, 2)\n",
    "print(f'Features: {x_predict}')\n",
    "\n",
    "log_reg_prediction = log_reg.predict(x_predict)\n",
    "print(f'Logistic Regression prediction: {log_reg_prediction}')\n",
    "\n",
    "svm_rbf_prediction = svm_rbf.predict(x_predict)\n",
    "print(f'RBF SVM prediction: {svm_rbf_prediction}')\n",
    "\n",
    "tree_clf_prediction = tree_clf.predict(x_predict)\n",
    "print(f'Decision Tree prediction: {tree_clf_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysis\n",
    "\n",
    "#### 2.4.1 Regularization: max depth\n",
    "\n",
    "We have successfully trained decision trees, but we haven't played with a different regularization hyperparameter: `max_depth`.\n",
    "\n",
    "`max_depth` \"cuts\" branches which are too long. i.e During training, nodes deeper than `max_depth` automatically become leaf nodes. \n",
    "\n",
    "Let's directly visualize the effect of this hyperparameter on the models's classification by plotting decision boundaries for different values of `max_depth`. We'll be using the handy `**kwargs` as arguments, here's a great [blog post](https://realpython.com/python-kwargs-and-args/) if you haven't heard about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:14:27.769069Z",
     "start_time": "2022-03-02T16:14:27.043652Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_tree(X, y, **kwargs):\n",
    "    tree_clf = DecisionTreeClassifier(random_state=0, **kwargs)\n",
    "    return tree_clf.fit(X, y)\n",
    "\n",
    "max_depth_values = [2, 5, 20]\n",
    "trees = [train_tree(X, y, max_depth=m) for m in max_depth_values]\n",
    "titles = [f'max_depth={max_depth}' for max_depth in max_depth_values]\n",
    "\n",
    "compare_classification(X, y, trees, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the regularizing effect of \"cutting off\" trees branches after a certain depth. `max_depth` has for effect of reducting the number of nested if-statements, and therefore limiting the number of \"angles\" in the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:23:10.031662Z",
     "start_time": "2022-03-02T16:23:10.028535Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = trees[2].predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:23:10.751411Z",
     "start_time": "2022-03-02T16:23:10.747161Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:23:11.290393Z",
     "start_time": "2022-03-02T16:23:11.285470Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:23:12.079240Z",
     "start_time": "2022-03-02T16:23:12.074195Z"
    }
   },
   "outputs": [],
   "source": [
    "recall_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:23:12.642254Z",
     "start_time": "2022-03-02T16:23:12.637079Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful though! Those metrics are being calculated on the training set, therefore they give no indication about the model's capacity to generalize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "Today we learned about two new supervised learning models: **naive bayes** and **decision trees**. We started by defining how naive bayes uses Baye's theorem to reframe classification problems as the estimation of a conditional probability. We described the model's tricks, namely: assuming the independence of the features, and assuming that they are Gaussian distributed. We explained how this allows to create fast and simple classifiers only by estimating Gaussian parameters and class priors. We then described the **decision tree** algorithm, and showed how it makes non-linear predictions with nested if-statements. We then went through its training procedure which uses **homogeneity metrics** or **variance reduction** to optimally split decision nodes. After noting these models' tendency to **overfit**, we introduced a **regularization** procedures: changing the trees' maximum depth.\n",
    "Finally, we applied naive bayes and decision trees to the banknote classification dataset.  We trained and visualized the models, as well as analysing the effect of regularization parameters.\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Core Resources\n",
    "\n",
    "\n",
    "- [sklearn naive bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [sklearn decision trees](https://scikit-learn.org/stable/modules/tree.html)  \n",
    "Official documentation about the tree package, handy breakdown of tree models in sklearn\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Understanding gini impurity](https://victorzhou.com/blog/gini-impurity/)  \n",
    "Same blog from victor zhou going into the mathematics of gini impurity\n",
    "- [args and kwargs demystified](https://realpython.com/python-kwargs-and-args/)  \n",
    "blog post about \\*\\*kwargs in python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
